# Generative_Images_with_VQ-VAE
This repo utilizes a Vector-Quantized Variational Autoencoder (VQ-VAE) trained on the FFHQ Dataset to generate human faces.

* Generate.ipynb
    >
    > This notebook contains all the code for the entire project, including code for the VQ-VAE+Transformer, training, as well as generation. Simply installing the required packages and running the notebook from the top should replicate results.
    >
    >VQ-VAE was trained for 69 epochs, transformer was trained for 53 epochs in a latent field of 16x16 with dimension size of 512. All samples are generated with temperature t = 1.0 and top-k sampling with k = 100. The interpolations are generated by progressively masking indices (original, 35%, 50%, 70%, 80%, 90%, 100% (i.e. new sample)). 

Best results:
![best](https://github.com/re8423/Generative_Images_with_VQ-VAE/blob/e3025c07d23db656140c95944f4d7ef87e39a7b0/Gen_samples/top_4.png)

Generated Samples: 
![samp](https://github.com/re8423/Generative_Images_with_VQ-VAE/blob/e3025c07d23db656140c95944f4d7ef87e39a7b0/Gen_samples/samples.png)

Interpolated results:
![inter](https://github.com/re8423/Generative_Images_with_VQ-VAE/blob/e3025c07d23db656140c95944f4d7ef87e39a7b0/Gen_samples/interpolation.png)
